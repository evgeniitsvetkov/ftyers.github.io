## Отчет по сегментации

Для сегментации предложений я использовал Punkt Sentence Tokenizer из библиотеки NLTK и модуль из библиотеку spaCy.

Punkt Sentence Tokenizer делит текст на предложения, используя алгоритм машинного обучение без учителя, который строит модель сокращений, коллокаций и слов, с которых начинаются предложения. Перед применением его необходимо предварительно обучить на большой коллекции документов соответствующего языка.

В spaCy для определения границ предложения используется анализ на основе деревьев зависимостей (dependency parsing). Такой подход зачастую дает более точный результат, чем подход основанный на правилах, но требует статистической модели. Для текстов новостей и статей он неплохо работает даже в базовой реализации. Для текстов из соцсетей или разговоров, где сложно выделить паттерны, может заметно превзойти решение, построенное на правилах. Позволяет подключить к пайплайну обработки текста компонент на основе правил с пользовательскими настройками (SentenceSegmenter).